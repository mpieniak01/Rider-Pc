# Rider-PC Client Configuration Example
# Copy this file to .env and customize the values

# Rider-PI Connection Settings
# Replace with your Rider-PI device IP address
RIDER_PI_HOST=192.168.1.14
RIDER_PI_PORT=8080
# Set true to disable Rider-PI REST/ZMQ integration (standalone mode)
DISABLE_RIDER_PI_ADAPTER=false

# ZMQ Configuration
# Ports for ZMQ PUB/SUB communication
ZMQ_PUB_PORT=5555
ZMQ_SUB_PORT=5556

# Local Server Configuration
# The PC client will run on these settings
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
# Public URL reachable from Rider-Pi (set to your PC IP + panel port)
PC_PUBLIC_BASE_URL=http://192.168.1.179:8080

# Cache Configuration
# SQLite database path and TTL
CACHE_DB_PATH=data/cache.db
CACHE_TTL_SECONDS=30

# Logging Level
# Options: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Provider Configuration
# Enable AI providers (Voice, Vision, Text)
ENABLE_PROVIDERS=false
VOICE_MODEL=mock
VISION_MODEL=mock
TEXT_MODEL=mock

# Provider Offload Configuration
# Enable offloading tasks to local providers
ENABLE_VISION_OFFLOAD=false
ENABLE_VOICE_OFFLOAD=true
# Enable text offload for Chat PC Standalone (requires ENABLE_PROVIDERS=true)
ENABLE_TEXT_OFFLOAD=true

# Provider Configuration Files
# Custom paths to provider configuration files
# VISION_PROVIDER_CONFIG=config/providers.toml
# VOICE_PROVIDER_CONFIG=config/providers.toml
# TEXT_PROVIDER_CONFIG=config/providers_text_local.toml

# Task Queue Configuration
# Enable asynchronous task queue
ENABLE_TASK_QUEUE=false
TASK_QUEUE_BACKEND=redis
TASK_QUEUE_HOST=localhost
TASK_QUEUE_PORT=6379
TASK_QUEUE_PASSWORD=
TASK_QUEUE_MAX_SIZE=100

# Telemetry Configuration
# Enable ZMQ telemetry publishing
ENABLE_TELEMETRY=false
TELEMETRY_ZMQ_HOST=0.0.0.0
TELEMETRY_ZMQ_PORT=5557

# Network Security Configuration
# Enable secure mode with mTLS (default: false for development)
SECURE_MODE=false
# Paths to mTLS certificates (only required when SECURE_MODE=true)
# Uncomment and set these paths when using secure mode:
# MTLS_CERT_PATH=/path/to/client-cert.pem
# MTLS_KEY_PATH=/path/to/client-key.pem
# MTLS_CA_PATH=/path/to/ca-cert.pem

# Local System Service Management
# Comma-separated list of systemd units to monitor (Linux only)
# Example:
# MONITORED_SERVICES=rider-pc.service,rider-voice.service,rider-task-queue.service
MONITORED_SERVICES=
# Whether to use sudo for systemctl commands (default: true)
# Set to false if running as root
SYSTEMD_USE_SUDO=true

# GitHub API Configuration
# Required for Project Dashboard feature
# Generate token at: https://github.com/settings/tokens
# Token needs 'repo' scope for private repos or 'public_repo' for public repos
# Set these in your shell environment for security (~/.bashrc or ~/.zshrc)
# GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GITHUB_TOKEN=
# Repository owner (username or organization)
GITHUB_REPO_OWNER=
# Repository name
GITHUB_REPO_NAME=
# Cache TTL in seconds for GitHub API responses (default: 300 = 5 minutes)
GITHUB_CACHE_TTL_SECONDS=300

# Task Auto-Init Configuration
# Settings for automatic task initialization (branch + documentation file creation)
# Enable/disable auto-init feature (default: true)
TASK_AUTO_INIT_ENABLED=true
# Path to task documentation directory (default: docs_pl/_to_do)
TASK_DOCS_PATH=docs_pl/_to_do
# Prefix for new branches (default: feat). Result: feat/<issue-number>-<title-slug>
TASK_BRANCH_PREFIX=feat

# RAG (Knowledge Base) Configuration
# Enable RAG for intelligent document search (requires chromadb and sentence-transformers)
RAG_ENABLED=false
# Comma-separated list of documentation directories to index
RAG_DOCS_PATHS=docs_pl,docs
# Sentence-transformers embedding model name
EMBEDDING_MODEL=all-MiniLM-L6-v2
# Chunk size for text splitting (in characters)
RAG_CHUNK_SIZE=800
# Overlap between chunks (in characters)
RAG_CHUNK_OVERLAP=100
# Path for persistent ChromaDB storage
RAG_PERSIST_PATH=data/chroma_db

# Google Assistant API Configuration
# Enable Google Assistant integration (requires OAuth setup and google-assistant-sdk)
GOOGLE_ASSISTANT_ENABLED=false
# Enable test mode (uses mock responses instead of real API calls)
GOOGLE_ASSISTANT_TEST_MODE=false
# Path to device configuration file (static map of devices and commands)
GOOGLE_ASSISTANT_DEVICES_CONFIG=config/google_assistant_devices.toml
# Path to OAuth tokens file (stored locally, should be in .gitignore)
# GOOGLE_ASSISTANT_TOKENS_PATH=config/local/google_assistant_tokens.json
# Google Cloud project ID (from Actions on Google console)
# GOOGLE_ASSISTANT_PROJECT_ID=your-project-id
# OAuth client ID (from Google Cloud Console)
# GOOGLE_ASSISTANT_CLIENT_ID=your-client-id.apps.googleusercontent.com
# OAuth client secret (from Google Cloud Console)
# GOOGLE_ASSISTANT_CLIENT_SECRET=your-client-secret
# Device model & instance IDs registered in Actions on Google (required for live mode)
# GOOGLE_ASSISTANT_DEVICE_MODEL_ID=rider-pc-panel-model
# GOOGLE_ASSISTANT_DEVICE_ID=rider-pc-panel-device
# Preferred language for Assistant queries (e.g., pl-PL, en-US)
# GOOGLE_ASSISTANT_LANGUAGE=pl-PL
# MCP (Model Context Protocol) Configuration
# Enable standalone MCP server mode (separate Uvicorn instance)
MCP_STANDALONE=false
# Port for standalone MCP server (used only when MCP_STANDALONE=true)
MCP_PORT=8210

# OpenWeather API Configuration (for MCP weather.get_summary tool)
# OpenWeather API key (get free key at: https://openweathermap.org/api)
# Leave empty to use mock weather data
OPENWEATHER_API_KEY=
# Cache TTL for weather data in seconds (default: 300 = 5 minutes)
WEATHER_CACHE_TTL_SECONDS=300
# Default location for weather queries (city,country_code format)
WEATHER_DEFAULT_LOCATION=Warsaw,PL

# Gemini API Configuration
# Get API key at: https://aistudio.google.com/app/apikey
# Leave empty to disable Gemini backend
GEMINI_API_KEY=
# Model name (e.g., gemini-2.0-flash, gemini-2.5-flash-lite)
GEMINI_MODEL=gemini-2.0-flash
# API endpoint (default: Google's generativelanguage API)
# GEMINI_ENDPOINT=https://generativelanguage.googleapis.com/v1beta

# OpenAI ChatGPT API Configuration
# Get API key at: https://platform.openai.com/api-keys
# Leave empty to disable ChatGPT backend
OPENAI_API_KEY=
# Model name (e.g., gpt-4o-mini, gpt-4o, gpt-4-turbo)
OPENAI_MODEL=gpt-4o-mini
# API base URL (change for Azure OpenAI or compatible endpoints)
# OPENAI_BASE_URL=https://api.openai.com/v1

# Text Provider Backend Configuration
# Which backend to use for text generation in Chat PC
# Options: local (Ollama), gemini, chatgpt, auto
# "auto" tries backends in order: local -> gemini -> chatgpt
TEXT_PROVIDER_BACKEND=local
